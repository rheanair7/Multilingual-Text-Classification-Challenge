{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Archive:  /content/drive/MyDrive/dev_phase.zip\n",
      "   creating: subtask1/\n",
      "   creating: subtask1/dev/\n",
      "  inflating: subtask1/dev/nep.csv    \n",
      "  inflating: subtask1/dev/ita.csv    \n",
      "  inflating: subtask1/dev/hin.csv    \n",
      "  inflating: subtask1/dev/hau.csv    \n",
      "  inflating: subtask1/dev/spa.csv    \n",
      "  inflating: subtask1/dev/deu.csv    \n",
      "  inflating: subtask1/dev/fas.csv    \n",
      "  inflating: subtask1/dev/arb.csv    \n",
      "  inflating: subtask1/dev/amh.csv    \n",
      "  inflating: subtask1/dev/tur.csv    \n",
      "  inflating: subtask1/dev/zho.csv    \n",
      "  inflating: subtask1/dev/eng.csv    \n",
      "  inflating: subtask1/dev/urd.csv    \n",
      "   creating: subtask1/train/\n",
      "  inflating: subtask1/train/nep.csv  \n",
      "  inflating: subtask1/train/ita.csv  \n",
      "  inflating: subtask1/train/hin.csv  \n",
      "  inflating: subtask1/train/fas.csv  \n",
      "  inflating: subtask1/train/deu.csv  \n",
      "  inflating: subtask1/train/hau.csv  \n",
      "  inflating: subtask1/train/spa.csv  \n",
      "  inflating: subtask1/train/arb.csv  \n",
      "  inflating: subtask1/train/tur.csv  \n",
      "  inflating: subtask1/train/zho.csv  \n",
      "  inflating: subtask1/train/amh.csv  \n",
      "  inflating: subtask1/train/urd.csv  \n",
      "  inflating: subtask1/train/eng.csv  \n",
      "   creating: subtask2/\n",
      "   creating: subtask2/train/\n",
      "  inflating: subtask2/train/nep.csv  \n",
      "  inflating: subtask2/train/ita.csv  \n",
      "  inflating: subtask2/train/hin.csv  \n",
      "  inflating: subtask2/train/deu.csv  \n",
      "  inflating: subtask2/train/fas.csv  \n",
      "  inflating: subtask2/train/hau.csv  \n",
      "  inflating: subtask2/train/spa.csv  \n",
      "  inflating: subtask2/train/arb.csv  \n",
      "  inflating: subtask2/train/amh.csv  \n",
      "  inflating: subtask2/train/zho.csv  \n",
      "  inflating: subtask2/train/tur.csv  \n",
      "  inflating: subtask2/train/urd.csv  \n",
      "  inflating: subtask2/train/eng.csv  \n",
      "   creating: subtask2/dev/\n",
      "  inflating: subtask2/dev/ita.csv    \n",
      "  inflating: subtask2/dev/nep.csv    \n",
      "  inflating: subtask2/dev/fas.csv    \n",
      "  inflating: subtask2/dev/deu.csv    \n",
      "  inflating: subtask2/dev/spa.csv    \n",
      "  inflating: subtask2/dev/hau.csv    \n",
      "  inflating: subtask2/dev/hin.csv    \n",
      "  inflating: subtask2/dev/tur.csv    \n",
      "  inflating: subtask2/dev/zho.csv    \n",
      "  inflating: subtask2/dev/amh.csv    \n",
      "  inflating: subtask2/dev/arb.csv    \n",
      "  inflating: subtask2/dev/urd.csv    \n",
      "  inflating: subtask2/dev/eng.csv    \n",
      "   creating: subtask3/\n",
      "   creating: subtask3/dev/\n",
      "  inflating: subtask3/dev/eng.csv    \n",
      "  inflating: subtask3/dev/urd.csv    \n",
      "  inflating: subtask3/dev/arb.csv    \n",
      "  inflating: subtask3/dev/amh.csv    \n",
      "  inflating: subtask3/dev/tur.csv    \n",
      "  inflating: subtask3/dev/zho.csv    \n",
      "  inflating: subtask3/dev/hin.csv    \n",
      "  inflating: subtask3/dev/spa.csv    \n",
      "  inflating: subtask3/dev/hau.csv    \n",
      "  inflating: subtask3/dev/deu.csv    \n",
      "  inflating: subtask3/dev/fas.csv    \n",
      "  inflating: subtask3/dev/nep.csv    \n",
      "   creating: subtask3/train/\n",
      "  inflating: subtask3/train/amh.csv  \n",
      "  inflating: subtask3/train/zho.csv  \n",
      "  inflating: subtask3/train/tur.csv  \n",
      "  inflating: subtask3/train/arb.csv  \n",
      "  inflating: subtask3/train/urd.csv  \n",
      "  inflating: subtask3/train/eng.csv  \n",
      "  inflating: subtask3/train/nep.csv  \n",
      "  inflating: subtask3/train/deu.csv  \n",
      "  inflating: subtask3/train/fas.csv  \n",
      "  inflating: subtask3/train/hau.csv  \n",
      "  inflating: subtask3/train/spa.csv  \n",
      "  inflating: subtask3/train/hin.csv  \n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "!unzip /content/drive/MyDrive/dev_phase.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/wozuk3b2?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7eb0fc7d50d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# Disable wandb logging for this script\n",
    "wandb.init(mode=\"disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PD1yg5hZVl4h"
   },
   "source": [
    "LEARNING RATE :2e-5 , EPOCH : 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1614185f75c741e8b542653a00df8918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "757b334521024cf9b3aa3da809011259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "073e95e848914f7c9ceaabe00cc41128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================\n",
      " LANGUAGE: eng\n",
      "Train size: 2140,  Validation size: 536\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "559825d487bc45768a096437b76ad3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf87da7a1938406d9a874b49ee974d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='170' max='170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [170/170 01:14, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.631100</td>\n",
       "      <td>0.525668</td>\n",
       "      <td>0.702075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.485400</td>\n",
       "      <td>0.484988</td>\n",
       "      <td>0.772796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.410400</td>\n",
       "      <td>0.466146</td>\n",
       "      <td>0.787869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.356800</td>\n",
       "      <td>0.465547</td>\n",
       "      <td>0.785753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.323900</td>\n",
       "      <td>0.460577</td>\n",
       "      <td>0.788822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " eng Validation F1 = 0.7888\n",
      " Predicting for dev set (133 rows)...\n",
      "\n",
      "====================================\n",
      " LANGUAGE: hin\n",
      "Train size: 2195,  Validation size: 549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [175/175 01:18, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.578000</td>\n",
       "      <td>0.373643</td>\n",
       "      <td>0.460707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.342000</td>\n",
       "      <td>0.326982</td>\n",
       "      <td>0.701720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.246300</td>\n",
       "      <td>0.316738</td>\n",
       "      <td>0.736722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.215600</td>\n",
       "      <td>0.312341</td>\n",
       "      <td>0.765440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.202700</td>\n",
       "      <td>0.314117</td>\n",
       "      <td>0.756033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hin Validation F1 = 0.7560\n",
      " Predicting for dev set (137 rows)...\n",
      "\n",
      "====================================\n",
      " LANGUAGE: spa\n",
      "Train size: 2644,  Validation size: 661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 01:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.677400</td>\n",
       "      <td>0.624180</td>\n",
       "      <td>0.685928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.568400</td>\n",
       "      <td>0.570306</td>\n",
       "      <td>0.716923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.487500</td>\n",
       "      <td>0.557844</td>\n",
       "      <td>0.722878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.440300</td>\n",
       "      <td>0.571750</td>\n",
       "      <td>0.730556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.412800</td>\n",
       "      <td>0.582611</td>\n",
       "      <td>0.726920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " spa Validation F1 = 0.7269\n",
      " Predicting for dev set (165 rows)...\n",
      "\n",
      "====================================\n",
      " LANGUAGE: urd\n",
      "Train size: 2279,  Validation size: 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180/180 01:20, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.662200</td>\n",
       "      <td>0.540939</td>\n",
       "      <td>0.694901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.488700</td>\n",
       "      <td>0.488820</td>\n",
       "      <td>0.730283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.439500</td>\n",
       "      <td>0.510893</td>\n",
       "      <td>0.733649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.381500</td>\n",
       "      <td>0.589515</td>\n",
       "      <td>0.696984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.359800</td>\n",
       "      <td>0.572799</td>\n",
       "      <td>0.704494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " urd Validation F1 = 0.7045\n",
      " Predicting for dev set (142 rows)...\n",
      "\n",
      "====================================\n",
      " LANGUAGE: zho\n",
      "Train size: 3424,  Validation size: 856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='270' max='270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [270/270 01:58, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.599400</td>\n",
       "      <td>0.484117</td>\n",
       "      <td>0.803477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.374500</td>\n",
       "      <td>0.371960</td>\n",
       "      <td>0.848128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.293700</td>\n",
       "      <td>0.368779</td>\n",
       "      <td>0.856238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.254100</td>\n",
       "      <td>0.381828</td>\n",
       "      <td>0.858622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.179400</td>\n",
       "      <td>0.385785</td>\n",
       "      <td>0.858504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " zho Validation F1 = 0.8585\n",
      " Predicting for dev set (214 rows)...\n",
      "\n",
      "====================================\n",
      " LANGUAGE: arb\n",
      "Train size: 2704,  Validation size: 676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='215' max='215' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [215/215 01:35, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.512100</td>\n",
       "      <td>0.514282</td>\n",
       "      <td>0.737506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.418000</td>\n",
       "      <td>0.479710</td>\n",
       "      <td>0.773128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.337800</td>\n",
       "      <td>0.507187</td>\n",
       "      <td>0.767197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.286400</td>\n",
       "      <td>0.518066</td>\n",
       "      <td>0.768954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.252300</td>\n",
       "      <td>0.542999</td>\n",
       "      <td>0.768262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " arb Validation F1 = 0.7683\n",
      " Predicting for dev set (169 rows)...\n",
      " Saved: eng_dev_predicted.csv\n",
      " Saved: hin_dev_predicted.csv\n",
      " Saved: spa_dev_predicted.csv\n",
      " Saved: urd_dev_predicted.csv\n",
      " Saved: zho_dev_predicted.csv\n",
      " Saved: arb_dev_predicted.csv\n",
      "\n",
      " FINAL F1 SCORES:\n",
      "  language  f1_macro\n",
      "0      eng  0.788822\n",
      "1      hin  0.756033\n",
      "2      spa  0.726920\n",
      "3      urd  0.704494\n",
      "4      zho  0.858504\n",
      "5      arb  0.768262\n"
     ]
    }
   ],
   "source": [
    "\n",
    "drive.mount('/content/drive')\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# ---------------------------------------\n",
    "# Dataset class\n",
    "# ---------------------------------------\n",
    "class PolarizationDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, require_labels=True):\n",
    "        self.texts = df[\"text\"].fillna(\"\").tolist()\n",
    "        if require_labels:\n",
    "            self.labels = df[\"polarization\"].astype(int).tolist()\n",
    "        else:\n",
    "            self.labels = [0] * len(self.texts)  # dummy labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k,v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# ---------------------------------------\n",
    "# Load data\n",
    "# ---------------------------------------\n",
    "languages = [\"eng\",\"hin\",\"spa\",\"urd\",\"zho\",\"arb\"]\n",
    "data = {}\n",
    "\n",
    "for lang in languages:\n",
    "    train_df = pd.read_csv(f\"subtask1/train/{lang}.csv\")   # labeled\n",
    "    dev_df   = pd.read_csv(f\"subtask1/dev/{lang}.csv\")     # unlabeled\n",
    "    data[lang] = {\"train\": train_df, \"dev\": dev_df}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/mdeberta-v3-base\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# Metric\n",
    "# ---------------------------------------\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\"f1_macro\": f1_score(p.label_ids, preds, average=\"macro\")}\n",
    "\n",
    "# ---------------------------------------\n",
    "# MAIN LOOP: TRAIN/VAL SPLIT + DEV PREDICTION\n",
    "# ---------------------------------------\n",
    "f1_results = []\n",
    "predicted_outputs = {}\n",
    "\n",
    "for lang, dfs in data.items():\n",
    "    print(\"\\n====================================\")\n",
    "    print(f\" LANGUAGE: {lang}\")\n",
    "\n",
    "    train_df = dfs[\"train\"]\n",
    "    dev_df   = dfs[\"dev\"]\n",
    "\n",
    "    # 1️⃣ Filter ONLY labeled training rows\n",
    "    train_labeled = train_df.dropna(subset=[\"polarization\"]).reset_index(drop=True)\n",
    "\n",
    "    # 2️⃣ Split train into train/validation\n",
    "    train_split, val_split = train_test_split(\n",
    "        train_labeled,\n",
    "        test_size=0.20,\n",
    "        stratify=train_labeled[\"polarization\"],\n",
    "        random_state=42,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    print(f\"Train size: {len(train_split)},  Validation size: {len(val_split)}\")\n",
    "\n",
    "    train_dataset = PolarizationDataset(train_split, tokenizer, require_labels=True)\n",
    "    val_dataset   = PolarizationDataset(val_split,   tokenizer, require_labels=True)\n",
    "\n",
    "    # 3️⃣ Train model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"microsoft/mdeberta-v3-base\", num_labels=2\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./model_{lang}\",\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=5,\n",
    "        per_device_train_batch_size=64,\n",
    "        per_device_eval_batch_size=16,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        logging_steps=20\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer)\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # 4️⃣ Compute F1 on validation\n",
    "    metrics = trainer.evaluate()\n",
    "    f1 = metrics[\"eval_f1_macro\"]\n",
    "    print(f\" {lang} Validation F1 = {f1:.4f}\")\n",
    "\n",
    "    f1_results.append({\"language\": lang, \"f1_macro\": f1})\n",
    "\n",
    "    # 5️⃣ Predict on dev (UNLABELED)\n",
    "    print(f\" Predicting for dev set ({len(dev_df)} rows)...\")\n",
    "    dev_dataset = PolarizationDataset(dev_df, tokenizer, require_labels=False)\n",
    "    preds = trainer.predict(dev_dataset)\n",
    "    pred_labels = np.argmax(preds.predictions, axis=1)\n",
    "\n",
    "    dev_df[\"predicted_polarization\"] = pred_labels\n",
    "    predicted_outputs[lang] = dev_df\n",
    "\n",
    "# ---------------------------------------\n",
    "# SAVE PREDICTIONS\n",
    "# ---------------------------------------\n",
    "for lang, df_pred in predicted_outputs.items():\n",
    "    df_pred.to_csv(f\"{lang}_dev_predicted.csv\", index=False)\n",
    "    print(f\" Saved: {lang}_dev_predicted.csv\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# FINAL F1 SCORES\n",
    "# ---------------------------------------\n",
    "f1_df = pd.DataFrame(f1_results)\n",
    "print(\"\\n FINAL F1 SCORES:\")\n",
    "print(f1_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6czcXNSgZEBm"
   },
   "source": [
    "LEARNING RATE: 1e-5 EPOCH:5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================\n",
      " LANGUAGE: eng\n",
      "Train size: 2140,  Validation size: 536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='170' max='170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [170/170 01:13, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.649500</td>\n",
       "      <td>0.550940</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.520700</td>\n",
       "      <td>0.524046</td>\n",
       "      <td>0.495097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.479700</td>\n",
       "      <td>0.510029</td>\n",
       "      <td>0.764222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.450400</td>\n",
       "      <td>0.490972</td>\n",
       "      <td>0.790876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.431400</td>\n",
       "      <td>0.491663</td>\n",
       "      <td>0.791830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " eng Validation F1 = 0.7918\n",
      " Predicting for dev set (133 rows)...\n",
      "\n",
      "====================================\n",
      " LANGUAGE: hin\n",
      "Train size: 2195,  Validation size: 549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [175/175 01:18, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.698000</td>\n",
       "      <td>0.403355</td>\n",
       "      <td>0.460707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.389800</td>\n",
       "      <td>0.360659</td>\n",
       "      <td>0.460707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.317000</td>\n",
       "      <td>0.320147</td>\n",
       "      <td>0.614930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.289200</td>\n",
       "      <td>0.299303</td>\n",
       "      <td>0.720788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.285100</td>\n",
       "      <td>0.302403</td>\n",
       "      <td>0.724999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hin Validation F1 = 0.7250\n",
      " Predicting for dev set (137 rows)...\n",
      "\n",
      "====================================\n",
      " LANGUAGE: spa\n",
      "Train size: 2644,  Validation size: 661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 01:32, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.690800</td>\n",
       "      <td>0.679334</td>\n",
       "      <td>0.519762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.621200</td>\n",
       "      <td>0.615305</td>\n",
       "      <td>0.692662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.555600</td>\n",
       "      <td>0.584920</td>\n",
       "      <td>0.700010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.539500</td>\n",
       "      <td>0.579606</td>\n",
       "      <td>0.713172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.524500</td>\n",
       "      <td>0.576889</td>\n",
       "      <td>0.706230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " spa Validation F1 = 0.7062\n",
      " Predicting for dev set (165 rows)...\n",
      "\n",
      "====================================\n",
      " LANGUAGE: urd\n",
      "Train size: 2279,  Validation size: 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180/180 01:20, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.711100</td>\n",
       "      <td>0.595619</td>\n",
       "      <td>0.409326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.563100</td>\n",
       "      <td>0.516469</td>\n",
       "      <td>0.723111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.499800</td>\n",
       "      <td>0.492263</td>\n",
       "      <td>0.730867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.454400</td>\n",
       "      <td>0.499344</td>\n",
       "      <td>0.718200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.434900</td>\n",
       "      <td>0.498980</td>\n",
       "      <td>0.721408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " urd Validation F1 = 0.7214\n",
      " Predicting for dev set (142 rows)...\n",
      "\n",
      "====================================\n",
      " LANGUAGE: zho\n",
      "Train size: 3424,  Validation size: 856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='270' max='270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [270/270 01:58, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.671300</td>\n",
       "      <td>0.549092</td>\n",
       "      <td>0.748882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.452000</td>\n",
       "      <td>0.438433</td>\n",
       "      <td>0.825881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.358300</td>\n",
       "      <td>0.406347</td>\n",
       "      <td>0.829625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.322800</td>\n",
       "      <td>0.402044</td>\n",
       "      <td>0.832995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.294400</td>\n",
       "      <td>0.401512</td>\n",
       "      <td>0.835746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " zho Validation F1 = 0.8357\n",
      " Predicting for dev set (214 rows)...\n",
      "\n",
      "====================================\n",
      " LANGUAGE: arb\n",
      "Train size: 2704,  Validation size: 676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='215' max='215' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [215/215 01:35, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.582900</td>\n",
       "      <td>0.538088</td>\n",
       "      <td>0.724560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.459400</td>\n",
       "      <td>0.474885</td>\n",
       "      <td>0.770669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.403500</td>\n",
       "      <td>0.486779</td>\n",
       "      <td>0.759612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.367100</td>\n",
       "      <td>0.472108</td>\n",
       "      <td>0.779956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.349300</td>\n",
       "      <td>0.483933</td>\n",
       "      <td>0.768296</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " arb Validation F1 = 0.7683\n",
      " Predicting for dev set (169 rows)...\n",
      " Saved: eng_dev_predicted.csv\n",
      " Saved: hin_dev_predicted.csv\n",
      " Saved: spa_dev_predicted.csv\n",
      " Saved: urd_dev_predicted.csv\n",
      " Saved: zho_dev_predicted.csv\n",
      " Saved: arb_dev_predicted.csv\n",
      "\n",
      " FINAL F1 SCORES:\n",
      "  language  f1_macro\n",
      "0      eng  0.791830\n",
      "1      hin  0.724999\n",
      "2      spa  0.706230\n",
      "3      urd  0.721408\n",
      "4      zho  0.835746\n",
      "5      arb  0.768296\n"
     ]
    }
   ],
   "source": [
    "\n",
    "drive.mount('/content/drive')\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# ---------------------------------------\n",
    "# Dataset class\n",
    "# ---------------------------------------\n",
    "class PolarizationDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, require_labels=True):\n",
    "        self.texts = df[\"text\"].fillna(\"\").tolist()\n",
    "        if require_labels:\n",
    "            self.labels = df[\"polarization\"].astype(int).tolist()\n",
    "        else:\n",
    "            self.labels = [0] * len(self.texts)  # dummy labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k,v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# ---------------------------------------\n",
    "# Load data\n",
    "# ---------------------------------------\n",
    "languages = [\"eng\",\"hin\",\"spa\",\"urd\",\"zho\",\"arb\"]\n",
    "data = {}\n",
    "\n",
    "for lang in languages:\n",
    "    train_df = pd.read_csv(f\"subtask1/train/{lang}.csv\")   # labeled\n",
    "    dev_df   = pd.read_csv(f\"subtask1/dev/{lang}.csv\")     # unlabeled\n",
    "    data[lang] = {\"train\": train_df, \"dev\": dev_df}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/mdeberta-v3-base\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# Metric\n",
    "# ---------------------------------------\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\"f1_macro\": f1_score(p.label_ids, preds, average=\"macro\")}\n",
    "\n",
    "# ---------------------------------------\n",
    "# MAIN LOOP: TRAIN/VAL SPLIT + DEV PREDICTION\n",
    "# ---------------------------------------\n",
    "f1_results = []\n",
    "predicted_outputs = {}\n",
    "\n",
    "for lang, dfs in data.items():\n",
    "    print(\"\\n====================================\")\n",
    "    print(f\" LANGUAGE: {lang}\")\n",
    "\n",
    "    train_df = dfs[\"train\"]\n",
    "    dev_df   = dfs[\"dev\"]\n",
    "\n",
    "    # 1️⃣ Filter ONLY labeled training rows\n",
    "    train_labeled = train_df.dropna(subset=[\"polarization\"]).reset_index(drop=True)\n",
    "\n",
    "    # 2️⃣ Split train into train/validation\n",
    "    train_split, val_split = train_test_split(\n",
    "        train_labeled,\n",
    "        test_size=0.20,\n",
    "        stratify=train_labeled[\"polarization\"],\n",
    "        random_state=42,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    print(f\"Train size: {len(train_split)},  Validation size: {len(val_split)}\")\n",
    "\n",
    "    train_dataset = PolarizationDataset(train_split, tokenizer, require_labels=True)\n",
    "    val_dataset   = PolarizationDataset(val_split,   tokenizer, require_labels=True)\n",
    "\n",
    "    # 3️⃣ Train model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"microsoft/mdeberta-v3-base\", num_labels=2\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./model_{lang}\",\n",
    "        learning_rate=1e-5,\n",
    "        num_train_epochs=5,\n",
    "        per_device_train_batch_size=64,\n",
    "        per_device_eval_batch_size=16,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        logging_steps=20\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer)\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # 4️⃣ Compute F1 on validation\n",
    "    metrics = trainer.evaluate()\n",
    "    f1 = metrics[\"eval_f1_macro\"]\n",
    "    print(f\" {lang} Validation F1 = {f1:.4f}\")\n",
    "\n",
    "    f1_results.append({\"language\": lang, \"f1_macro\": f1})\n",
    "\n",
    "    # 5️⃣ Predict on dev (UNLABELED)\n",
    "    print(f\" Predicting for dev set ({len(dev_df)} rows)...\")\n",
    "    dev_dataset = PolarizationDataset(dev_df, tokenizer, require_labels=False)\n",
    "    preds = trainer.predict(dev_dataset)\n",
    "    pred_labels = np.argmax(preds.predictions, axis=1)\n",
    "\n",
    "    dev_df[\"predicted_polarization\"] = pred_labels\n",
    "    predicted_outputs[lang] = dev_df\n",
    "\n",
    "# ---------------------------------------\n",
    "# SAVE PREDICTIONS\n",
    "# ---------------------------------------\n",
    "for lang, df_pred in predicted_outputs.items():\n",
    "    df_pred.to_csv(f\"{lang}_dev_predicted.csv\", index=False)\n",
    "    print(f\" Saved: {lang}_dev_predicted.csv\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# FINAL F1 SCORES\n",
    "# ---------------------------------------\n",
    "f1_df = pd.DataFrame(f1_results)\n",
    "print(\"\\n FINAL F1 SCORES:\")\n",
    "print(f1_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================\n",
      " LANGUAGE: eng\n",
      "Train size: 2140,  Validation size: 536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='340' max='340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [340/340 02:27, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.649300</td>\n",
       "      <td>0.548584</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.518500</td>\n",
       "      <td>0.519991</td>\n",
       "      <td>0.752472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.465300</td>\n",
       "      <td>0.512483</td>\n",
       "      <td>0.765174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.427900</td>\n",
       "      <td>0.476034</td>\n",
       "      <td>0.771526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.395100</td>\n",
       "      <td>0.507187</td>\n",
       "      <td>0.777858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.350200</td>\n",
       "      <td>0.496946</td>\n",
       "      <td>0.787183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.310800</td>\n",
       "      <td>0.493687</td>\n",
       "      <td>0.788201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.297300</td>\n",
       "      <td>0.501605</td>\n",
       "      <td>0.788201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.285400</td>\n",
       "      <td>0.507672</td>\n",
       "      <td>0.792088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.263800</td>\n",
       "      <td>0.513283</td>\n",
       "      <td>0.783977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " eng Validation F1 = 0.7840\n",
      " Predicting for dev set (133 rows)...\n",
      "\n",
      "====================================\n",
      " LANGUAGE: hin\n",
      "Train size: 2195,  Validation size: 549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='350' max='350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [350/350 02:37, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.531600</td>\n",
       "      <td>0.404365</td>\n",
       "      <td>0.460707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.389800</td>\n",
       "      <td>0.386658</td>\n",
       "      <td>0.460707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.331400</td>\n",
       "      <td>0.343277</td>\n",
       "      <td>0.646393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.294600</td>\n",
       "      <td>0.331015</td>\n",
       "      <td>0.642757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.263000</td>\n",
       "      <td>0.307892</td>\n",
       "      <td>0.710624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.207500</td>\n",
       "      <td>0.308145</td>\n",
       "      <td>0.753190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.194500</td>\n",
       "      <td>0.308011</td>\n",
       "      <td>0.737468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.171900</td>\n",
       "      <td>0.313227</td>\n",
       "      <td>0.743592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.186800</td>\n",
       "      <td>0.314294</td>\n",
       "      <td>0.741247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.152800</td>\n",
       "      <td>0.316457</td>\n",
       "      <td>0.737468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hin Validation F1 = 0.7375\n",
      " Predicting for dev set (137 rows)...\n",
      "\n",
      "====================================\n",
      " LANGUAGE: spa\n",
      "Train size: 2644,  Validation size: 661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='420' max='420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [420/420 03:04, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.691500</td>\n",
       "      <td>0.687047</td>\n",
       "      <td>0.644224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.599400</td>\n",
       "      <td>0.564916</td>\n",
       "      <td>0.715596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.506300</td>\n",
       "      <td>0.550599</td>\n",
       "      <td>0.727230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.463400</td>\n",
       "      <td>0.556177</td>\n",
       "      <td>0.750321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.416600</td>\n",
       "      <td>0.560209</td>\n",
       "      <td>0.754912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.341400</td>\n",
       "      <td>0.637651</td>\n",
       "      <td>0.737698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.346100</td>\n",
       "      <td>0.618278</td>\n",
       "      <td>0.742681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.316100</td>\n",
       "      <td>0.654075</td>\n",
       "      <td>0.739057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.292500</td>\n",
       "      <td>0.644911</td>\n",
       "      <td>0.745629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.270200</td>\n",
       "      <td>0.660517</td>\n",
       "      <td>0.740693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " spa Validation F1 = 0.7407\n",
      " Predicting for dev set (165 rows)...\n",
      "\n",
      "====================================\n",
      " LANGUAGE: urd\n",
      "Train size: 2279,  Validation size: 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='360' max='360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [360/360 02:41, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.635400</td>\n",
       "      <td>0.585910</td>\n",
       "      <td>0.409326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.542600</td>\n",
       "      <td>0.510875</td>\n",
       "      <td>0.698058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.466600</td>\n",
       "      <td>0.533393</td>\n",
       "      <td>0.711149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.404100</td>\n",
       "      <td>0.528062</td>\n",
       "      <td>0.709800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.356500</td>\n",
       "      <td>0.596112</td>\n",
       "      <td>0.692792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.325600</td>\n",
       "      <td>0.588718</td>\n",
       "      <td>0.700725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.278600</td>\n",
       "      <td>0.628294</td>\n",
       "      <td>0.700226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.274700</td>\n",
       "      <td>0.598247</td>\n",
       "      <td>0.712454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.246200</td>\n",
       "      <td>0.639238</td>\n",
       "      <td>0.710293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.236500</td>\n",
       "      <td>0.655661</td>\n",
       "      <td>0.707146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " urd Validation F1 = 0.7071\n",
      " Predicting for dev set (142 rows)...\n",
      "\n",
      "====================================\n",
      " LANGUAGE: zho\n",
      "Train size: 3424,  Validation size: 856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='540' max='540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [540/540 03:58, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.640300</td>\n",
       "      <td>0.523766</td>\n",
       "      <td>0.772496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.459000</td>\n",
       "      <td>0.428475</td>\n",
       "      <td>0.830579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.341200</td>\n",
       "      <td>0.367117</td>\n",
       "      <td>0.850460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.312900</td>\n",
       "      <td>0.367456</td>\n",
       "      <td>0.850721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.260500</td>\n",
       "      <td>0.375407</td>\n",
       "      <td>0.859787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.207700</td>\n",
       "      <td>0.382524</td>\n",
       "      <td>0.859847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.197700</td>\n",
       "      <td>0.373274</td>\n",
       "      <td>0.868711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>0.381128</td>\n",
       "      <td>0.866290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.154800</td>\n",
       "      <td>0.392941</td>\n",
       "      <td>0.869101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.165700</td>\n",
       "      <td>0.392194</td>\n",
       "      <td>0.867817</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " zho Validation F1 = 0.8678\n",
      " Predicting for dev set (214 rows)...\n",
      "\n",
      "====================================\n",
      " LANGUAGE: arb\n",
      "Train size: 2704,  Validation size: 676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='430' max='430' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [430/430 03:11, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.599000</td>\n",
       "      <td>0.573268</td>\n",
       "      <td>0.690748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.447100</td>\n",
       "      <td>0.472267</td>\n",
       "      <td>0.770227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.383900</td>\n",
       "      <td>0.478639</td>\n",
       "      <td>0.776603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.317700</td>\n",
       "      <td>0.489504</td>\n",
       "      <td>0.789676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.294000</td>\n",
       "      <td>0.513855</td>\n",
       "      <td>0.771557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.283200</td>\n",
       "      <td>0.526995</td>\n",
       "      <td>0.771389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.261000</td>\n",
       "      <td>0.582942</td>\n",
       "      <td>0.761664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.219400</td>\n",
       "      <td>0.570255</td>\n",
       "      <td>0.776886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.185600</td>\n",
       "      <td>0.585845</td>\n",
       "      <td>0.778605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.196000</td>\n",
       "      <td>0.596242</td>\n",
       "      <td>0.776681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " arb Validation F1 = 0.7767\n",
      " Predicting for dev set (169 rows)...\n",
      " Saved: eng_dev_predicted.csv\n",
      " Saved: hin_dev_predicted.csv\n",
      " Saved: spa_dev_predicted.csv\n",
      " Saved: urd_dev_predicted.csv\n",
      " Saved: zho_dev_predicted.csv\n",
      " Saved: arb_dev_predicted.csv\n",
      "\n",
      " FINAL F1 SCORES:\n",
      "  language  f1_macro\n",
      "0      eng  0.783977\n",
      "1      hin  0.737468\n",
      "2      spa  0.740693\n",
      "3      urd  0.707146\n",
      "4      zho  0.867817\n",
      "5      arb  0.776681\n"
     ]
    }
   ],
   "source": [
    "\n",
    "drive.mount('/content/drive')\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# ---------------------------------------\n",
    "# Dataset class\n",
    "# ---------------------------------------\n",
    "class PolarizationDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, require_labels=True):\n",
    "        self.texts = df[\"text\"].fillna(\"\").tolist()\n",
    "        if require_labels:\n",
    "            self.labels = df[\"polarization\"].astype(int).tolist()\n",
    "        else:\n",
    "            self.labels = [0] * len(self.texts)  # dummy labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k,v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# ---------------------------------------\n",
    "# Load data\n",
    "# ---------------------------------------\n",
    "languages = [\"eng\",\"hin\",\"spa\",\"urd\",\"zho\",\"arb\"]\n",
    "data = {}\n",
    "\n",
    "for lang in languages:\n",
    "    train_df = pd.read_csv(f\"subtask1/train/{lang}.csv\")   # labeled\n",
    "    dev_df   = pd.read_csv(f\"subtask1/dev/{lang}.csv\")     # unlabeled\n",
    "    data[lang] = {\"train\": train_df, \"dev\": dev_df}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/mdeberta-v3-base\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# Metric\n",
    "# ---------------------------------------\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\"f1_macro\": f1_score(p.label_ids, preds, average=\"macro\")}\n",
    "\n",
    "# ---------------------------------------\n",
    "# MAIN LOOP: TRAIN/VAL SPLIT + DEV PREDICTION\n",
    "# ---------------------------------------\n",
    "f1_results = []\n",
    "predicted_outputs = {}\n",
    "\n",
    "for lang, dfs in data.items():\n",
    "    print(\"\\n====================================\")\n",
    "    print(f\" LANGUAGE: {lang}\")\n",
    "\n",
    "    train_df = dfs[\"train\"]\n",
    "    dev_df   = dfs[\"dev\"]\n",
    "\n",
    "    # 1️⃣ Filter ONLY labeled training rows\n",
    "    train_labeled = train_df.dropna(subset=[\"polarization\"]).reset_index(drop=True)\n",
    "\n",
    "    # 2️⃣ Split train into train/validation\n",
    "    train_split, val_split = train_test_split(\n",
    "        train_labeled,\n",
    "        test_size=0.20,\n",
    "        stratify=train_labeled[\"polarization\"],\n",
    "        random_state=42,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    print(f\"Train size: {len(train_split)},  Validation size: {len(val_split)}\")\n",
    "\n",
    "    train_dataset = PolarizationDataset(train_split, tokenizer, require_labels=True)\n",
    "    val_dataset   = PolarizationDataset(val_split,   tokenizer, require_labels=True)\n",
    "\n",
    "    # 3️⃣ Train model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"microsoft/mdeberta-v3-base\", num_labels=2\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./model_{lang}\",\n",
    "        learning_rate=1e-5,\n",
    "        num_train_epochs=10,\n",
    "        per_device_train_batch_size=64,\n",
    "        per_device_eval_batch_size=16,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        logging_steps=20,\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer)\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # 4️⃣ Compute F1 on validation\n",
    "    metrics = trainer.evaluate()\n",
    "    f1 = metrics[\"eval_f1_macro\"]\n",
    "    print(f\" {lang} Validation F1 = {f1:.4f}\")\n",
    "\n",
    "    f1_results.append({\"language\": lang, \"f1_macro\": f1})\n",
    "\n",
    "    # 5️⃣ Predict on dev (UNLABELED)\n",
    "    print(f\" Predicting for dev set ({len(dev_df)} rows)...\")\n",
    "    dev_dataset = PolarizationDataset(dev_df, tokenizer, require_labels=False)\n",
    "    preds = trainer.predict(dev_dataset)\n",
    "    pred_labels = np.argmax(preds.predictions, axis=1)\n",
    "\n",
    "    dev_df[\"predicted_polarization\"] = pred_labels\n",
    "    predicted_outputs[lang] = dev_df\n",
    "\n",
    "# ---------------------------------------\n",
    "# SAVE PREDICTIONS\n",
    "# ---------------------------------------\n",
    "for lang, df_pred in predicted_outputs.items():\n",
    "    df_pred.to_csv(f\"{lang}_dev_predicted.csv\", index=False)\n",
    "    print(f\" Saved: {lang}_dev_predicted.csv\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# FINAL F1 SCORES\n",
    "# ---------------------------------------\n",
    "f1_df = pd.DataFrame(f1_results)\n",
    "print(\"\\n FINAL F1 SCORES:\")\n",
    "print(f1_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================\n",
      " LANGUAGE: eng\n",
      "Train size: 2140,  Validation size: 536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='170' max='170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [170/170 01:13, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.674300</td>\n",
       "      <td>0.575518</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.534500</td>\n",
       "      <td>0.520420</td>\n",
       "      <td>0.682765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.475200</td>\n",
       "      <td>0.497338</td>\n",
       "      <td>0.768257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.440200</td>\n",
       "      <td>0.474629</td>\n",
       "      <td>0.778252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.414900</td>\n",
       "      <td>0.469080</td>\n",
       "      <td>0.776085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " eng Validation F1 = 0.7761\n",
      " Predicting for dev set (133 rows)...\n",
      "\n",
      "====================================\n",
      " LANGUAGE: hin\n",
      "Train size: 2195,  Validation size: 549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [175/175 01:18, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.798900</td>\n",
       "      <td>0.410252</td>\n",
       "      <td>0.460707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.390800</td>\n",
       "      <td>0.358020</td>\n",
       "      <td>0.460707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.312800</td>\n",
       "      <td>0.317473</td>\n",
       "      <td>0.674844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.301805</td>\n",
       "      <td>0.735543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.279000</td>\n",
       "      <td>0.300744</td>\n",
       "      <td>0.739867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hin Validation F1 = 0.7399\n",
      " Predicting for dev set (137 rows)...\n",
      "\n",
      "====================================\n",
      " LANGUAGE: spa\n",
      "Train size: 2644,  Validation size: 661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='210' max='210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [210/210 01:32, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.693400</td>\n",
       "      <td>0.689585</td>\n",
       "      <td>0.557413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.637600</td>\n",
       "      <td>0.618649</td>\n",
       "      <td>0.689135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.565700</td>\n",
       "      <td>0.576970</td>\n",
       "      <td>0.688319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.540100</td>\n",
       "      <td>0.572387</td>\n",
       "      <td>0.723698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.517900</td>\n",
       "      <td>0.559405</td>\n",
       "      <td>0.711873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " spa Validation F1 = 0.7119\n",
      " Predicting for dev set (165 rows)...\n",
      "\n",
      "====================================\n",
      " LANGUAGE: urd\n",
      "Train size: 2279,  Validation size: 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [180/180 01:20, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.758200</td>\n",
       "      <td>0.581413</td>\n",
       "      <td>0.409326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.558000</td>\n",
       "      <td>0.538239</td>\n",
       "      <td>0.409326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.515500</td>\n",
       "      <td>0.526869</td>\n",
       "      <td>0.409326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.476800</td>\n",
       "      <td>0.504108</td>\n",
       "      <td>0.646668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.465000</td>\n",
       "      <td>0.506018</td>\n",
       "      <td>0.710498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " urd Validation F1 = 0.7105\n",
      " Predicting for dev set (142 rows)...\n",
      "\n",
      "====================================\n",
      " LANGUAGE: zho\n",
      "Train size: 3424,  Validation size: 856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='270' max='270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [270/270 01:59, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.693400</td>\n",
       "      <td>0.605140</td>\n",
       "      <td>0.755833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.480200</td>\n",
       "      <td>0.464657</td>\n",
       "      <td>0.806048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.370100</td>\n",
       "      <td>0.414098</td>\n",
       "      <td>0.829743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.315200</td>\n",
       "      <td>0.408223</td>\n",
       "      <td>0.835283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.273600</td>\n",
       "      <td>0.404882</td>\n",
       "      <td>0.838041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " zho Validation F1 = 0.8380\n",
      " Predicting for dev set (214 rows)...\n",
      "\n",
      "====================================\n",
      " LANGUAGE: arb\n",
      "Train size: 2704,  Validation size: 676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='215' max='215' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [215/215 01:35, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.646200</td>\n",
       "      <td>0.609240</td>\n",
       "      <td>0.656416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.473900</td>\n",
       "      <td>0.477260</td>\n",
       "      <td>0.772171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.402300</td>\n",
       "      <td>0.485606</td>\n",
       "      <td>0.757449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.366500</td>\n",
       "      <td>0.482987</td>\n",
       "      <td>0.766117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.339600</td>\n",
       "      <td>0.494079</td>\n",
       "      <td>0.765612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " arb Validation F1 = 0.7656\n",
      " Predicting for dev set (169 rows)...\n",
      " Saved: eng_dev_predicted.csv\n",
      " Saved: hin_dev_predicted.csv\n",
      " Saved: spa_dev_predicted.csv\n",
      " Saved: urd_dev_predicted.csv\n",
      " Saved: zho_dev_predicted.csv\n",
      " Saved: arb_dev_predicted.csv\n",
      "\n",
      " FINAL F1 SCORES:\n",
      "  language  f1_macro\n",
      "0      eng  0.776085\n",
      "1      hin  0.739867\n",
      "2      spa  0.711873\n",
      "3      urd  0.710498\n",
      "4      zho  0.838041\n",
      "5      arb  0.765612\n"
     ]
    }
   ],
   "source": [
    "\n",
    "drive.mount('/content/drive')\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# ---------------------------------------\n",
    "# Dataset class\n",
    "# ---------------------------------------\n",
    "class PolarizationDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, require_labels=True):\n",
    "        self.texts = df[\"text\"].fillna(\"\").tolist()\n",
    "        if require_labels:\n",
    "            self.labels = df[\"polarization\"].astype(int).tolist()\n",
    "        else:\n",
    "            self.labels = [0] * len(self.texts)  # dummy labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k,v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# ---------------------------------------\n",
    "# Load data\n",
    "# ---------------------------------------\n",
    "languages = [\"eng\",\"hin\",\"spa\",\"urd\",\"zho\",\"arb\"]\n",
    "data = {}\n",
    "\n",
    "for lang in languages:\n",
    "    train_df = pd.read_csv(f\"subtask1/train/{lang}.csv\")   # labeled\n",
    "    dev_df   = pd.read_csv(f\"subtask1/dev/{lang}.csv\")     # unlabeled\n",
    "    data[lang] = {\"train\": train_df, \"dev\": dev_df}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/mdeberta-v3-base\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# Metric\n",
    "# ---------------------------------------\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\"f1_macro\": f1_score(p.label_ids, preds, average=\"macro\")}\n",
    "\n",
    "# ---------------------------------------\n",
    "# MAIN LOOP: TRAIN/VAL SPLIT + DEV PREDICTION\n",
    "# ---------------------------------------\n",
    "f1_results = []\n",
    "predicted_outputs = {}\n",
    "\n",
    "for lang, dfs in data.items():\n",
    "    print(\"\\n====================================\")\n",
    "    print(f\" LANGUAGE: {lang}\")\n",
    "\n",
    "    train_df = dfs[\"train\"]\n",
    "    dev_df   = dfs[\"dev\"]\n",
    "\n",
    "    # 1️⃣ Filter ONLY labeled training rows\n",
    "    train_labeled = train_df.dropna(subset=[\"polarization\"]).reset_index(drop=True)\n",
    "\n",
    "    # 2️⃣ Split train into train/validation\n",
    "    train_split, val_split = train_test_split(\n",
    "        train_labeled,\n",
    "        test_size=0.20,\n",
    "        stratify=train_labeled[\"polarization\"],\n",
    "        random_state=42,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    print(f\"Train size: {len(train_split)},  Validation size: {len(val_split)}\")\n",
    "\n",
    "    train_dataset = PolarizationDataset(train_split, tokenizer, require_labels=True)\n",
    "    val_dataset   = PolarizationDataset(val_split,   tokenizer, require_labels=True)\n",
    "\n",
    "    # 3️⃣ Train model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"microsoft/mdeberta-v3-base\", num_labels=2\n",
    "    )\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "    output_dir=f\"./model_{lang}\",\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=16,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    logging_steps=20,\n",
    "\n",
    "    # added as you requested\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    ")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer)\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # 4️⃣ Compute F1 on validation\n",
    "    metrics = trainer.evaluate()\n",
    "    f1 = metrics[\"eval_f1_macro\"]\n",
    "    print(f\" {lang} Validation F1 = {f1:.4f}\")\n",
    "\n",
    "    f1_results.append({\"language\": lang, \"f1_macro\": f1})\n",
    "\n",
    "    # 5️⃣ Predict on dev (UNLABELED)\n",
    "    print(f\" Predicting for dev set ({len(dev_df)} rows)...\")\n",
    "    dev_dataset = PolarizationDataset(dev_df, tokenizer, require_labels=False)\n",
    "    preds = trainer.predict(dev_dataset)\n",
    "    pred_labels = np.argmax(preds.predictions, axis=1)\n",
    "\n",
    "    dev_df[\"predicted_polarization\"] = pred_labels\n",
    "    predicted_outputs[lang] = dev_df\n",
    "\n",
    "# ---------------------------------------\n",
    "# SAVE PREDICTIONS\n",
    "# ---------------------------------------\n",
    "for lang, df_pred in predicted_outputs.items():\n",
    "    df_pred.to_csv(f\"{lang}_dev_predicted.csv\", index=False)\n",
    "    print(f\" Saved: {lang}_dev_predicted.csv\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# FINAL F1 SCORES\n",
    "# ---------------------------------------\n",
    "f1_df = pd.DataFrame(f1_results)\n",
    "print(\"\\n FINAL F1 SCORES:\")\n",
    "print(f1_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================\n",
      " LANGUAGE: eng\n",
      "Train size: 2140,  Validation size: 536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='335' max='335' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [335/335 01:18, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.583300</td>\n",
       "      <td>0.534133</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.481100</td>\n",
       "      <td>0.475503</td>\n",
       "      <td>0.774326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.383300</td>\n",
       "      <td>0.519145</td>\n",
       "      <td>0.762127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.345100</td>\n",
       "      <td>0.464389</td>\n",
       "      <td>0.780857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.320500</td>\n",
       "      <td>0.474658</td>\n",
       "      <td>0.783193</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " eng Validation F1 = 0.7832\n",
      " Predicting for dev set (133 rows)...\n",
      "\n",
      "====================================\n",
      " LANGUAGE: hin\n",
      "Train size: 2195,  Validation size: 549\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='345' max='345' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [345/345 01:37, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.413400</td>\n",
       "      <td>0.389759</td>\n",
       "      <td>0.460707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.346100</td>\n",
       "      <td>0.305972</td>\n",
       "      <td>0.731442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.266800</td>\n",
       "      <td>0.302578</td>\n",
       "      <td>0.743138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.226300</td>\n",
       "      <td>0.316570</td>\n",
       "      <td>0.762816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.164400</td>\n",
       "      <td>0.315392</td>\n",
       "      <td>0.765854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " hin Validation F1 = 0.7659\n",
      " Predicting for dev set (137 rows)...\n",
      "\n",
      "====================================\n",
      " LANGUAGE: spa\n",
      "Train size: 2644,  Validation size: 661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='415' max='415' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [415/415 01:32, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.679500</td>\n",
       "      <td>0.643613</td>\n",
       "      <td>0.664312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.581900</td>\n",
       "      <td>0.553948</td>\n",
       "      <td>0.712890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.464000</td>\n",
       "      <td>0.549940</td>\n",
       "      <td>0.735644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.421400</td>\n",
       "      <td>0.559146</td>\n",
       "      <td>0.739057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.454500</td>\n",
       "      <td>0.562146</td>\n",
       "      <td>0.739415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " spa Validation F1 = 0.7394\n",
      " Predicting for dev set (165 rows)...\n",
      "\n",
      "====================================\n",
      " LANGUAGE: urd\n",
      "Train size: 2279,  Validation size: 570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='360' max='360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [360/360 01:28, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.608900</td>\n",
       "      <td>0.547224</td>\n",
       "      <td>0.409326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.536300</td>\n",
       "      <td>0.495450</td>\n",
       "      <td>0.528194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.457000</td>\n",
       "      <td>0.471344</td>\n",
       "      <td>0.745109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.429500</td>\n",
       "      <td>0.473527</td>\n",
       "      <td>0.737246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.400200</td>\n",
       "      <td>0.473931</td>\n",
       "      <td>0.741766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " urd Validation F1 = 0.7451\n",
      " Predicting for dev set (142 rows)...\n",
      "\n",
      "====================================\n",
      " LANGUAGE: zho\n",
      "Train size: 3424,  Validation size: 856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='535' max='535' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [535/535 01:57, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.505100</td>\n",
       "      <td>0.458759</td>\n",
       "      <td>0.797950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.392900</td>\n",
       "      <td>0.391890</td>\n",
       "      <td>0.836220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.297900</td>\n",
       "      <td>0.378026</td>\n",
       "      <td>0.849190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.203300</td>\n",
       "      <td>0.388466</td>\n",
       "      <td>0.861932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.208000</td>\n",
       "      <td>0.392325</td>\n",
       "      <td>0.862113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " zho Validation F1 = 0.8621\n",
      " Predicting for dev set (214 rows)...\n",
      "\n",
      "====================================\n",
      " LANGUAGE: arb\n",
      "Train size: 2704,  Validation size: 676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='425' max='425' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [425/425 01:41, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.519200</td>\n",
       "      <td>0.543641</td>\n",
       "      <td>0.701777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.414800</td>\n",
       "      <td>0.466359</td>\n",
       "      <td>0.782544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.335600</td>\n",
       "      <td>0.479337</td>\n",
       "      <td>0.773594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.301400</td>\n",
       "      <td>0.521434</td>\n",
       "      <td>0.774456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.280300</td>\n",
       "      <td>0.515198</td>\n",
       "      <td>0.765161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " arb Validation F1 = 0.7825\n",
      " Predicting for dev set (169 rows)...\n",
      " Saved: eng_dev_predicted.csv\n",
      " Saved: hin_dev_predicted.csv\n",
      " Saved: spa_dev_predicted.csv\n",
      " Saved: urd_dev_predicted.csv\n",
      " Saved: zho_dev_predicted.csv\n",
      " Saved: arb_dev_predicted.csv\n",
      "\n",
      " FINAL F1 SCORES:\n",
      "  language  f1_macro\n",
      "0      eng  0.783193\n",
      "1      hin  0.765854\n",
      "2      spa  0.739415\n",
      "3      urd  0.745109\n",
      "4      zho  0.862113\n",
      "5      arb  0.782544\n"
     ]
    }
   ],
   "source": [
    "\n",
    "drive.mount('/content/drive')\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# ---------------------------------------\n",
    "# Dataset class\n",
    "# ---------------------------------------\n",
    "class PolarizationDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, require_labels=True):\n",
    "        self.texts = df[\"text\"].fillna(\"\").tolist()\n",
    "        if require_labels:\n",
    "            self.labels = df[\"polarization\"].astype(int).tolist()\n",
    "        else:\n",
    "            self.labels = [0] * len(self.texts)  # dummy labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=256,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k,v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "# ---------------------------------------\n",
    "# Load data\n",
    "# ---------------------------------------\n",
    "languages = [\"eng\",\"hin\",\"spa\",\"urd\",\"zho\",\"arb\"]\n",
    "data = {}\n",
    "\n",
    "for lang in languages:\n",
    "    train_df = pd.read_csv(f\"subtask1/train/{lang}.csv\")   # labeled\n",
    "    dev_df   = pd.read_csv(f\"subtask1/dev/{lang}.csv\")     # unlabeled\n",
    "    data[lang] = {\"train\": train_df, \"dev\": dev_df}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/mdeberta-v3-base\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# Metric\n",
    "# ---------------------------------------\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {\"f1_macro\": f1_score(p.label_ids, preds, average=\"macro\")}\n",
    "\n",
    "# ---------------------------------------\n",
    "# MAIN LOOP: TRAIN/VAL SPLIT + DEV PREDICTION\n",
    "# ---------------------------------------\n",
    "f1_results = []\n",
    "predicted_outputs = {}\n",
    "\n",
    "for lang, dfs in data.items():\n",
    "    print(\"\\n====================================\")\n",
    "    print(f\" LANGUAGE: {lang}\")\n",
    "\n",
    "    train_df = dfs[\"train\"]\n",
    "    dev_df   = dfs[\"dev\"]\n",
    "\n",
    "    # 1️⃣ Filter ONLY labeled training rows\n",
    "    train_labeled = train_df.dropna(subset=[\"polarization\"]).reset_index(drop=True)\n",
    "\n",
    "    # 2️⃣ Split train into train/validation\n",
    "    train_split, val_split = train_test_split(\n",
    "        train_labeled,\n",
    "        test_size=0.20,\n",
    "        stratify=train_labeled[\"polarization\"],\n",
    "        random_state=42,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    print(f\"Train size: {len(train_split)},  Validation size: {len(val_split)}\")\n",
    "\n",
    "    train_dataset = PolarizationDataset(train_split, tokenizer, require_labels=True)\n",
    "    val_dataset   = PolarizationDataset(val_split,   tokenizer, require_labels=True)\n",
    "\n",
    "    # 3️⃣ Train model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"microsoft/mdeberta-v3-base\", num_labels=2\n",
    "    )\n",
    "    training_args = TrainingArguments(\n",
    "    output_dir=f\"./model_{lang}\",\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=16,\n",
    "\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    fp16=True,\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    logging_steps=20,\n",
    ")\n",
    "\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer)\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # 4️⃣ Compute F1 on validation\n",
    "    metrics = trainer.evaluate()\n",
    "    f1 = metrics[\"eval_f1_macro\"]\n",
    "    print(f\" {lang} Validation F1 = {f1:.4f}\")\n",
    "\n",
    "    f1_results.append({\"language\": lang, \"f1_macro\": f1})\n",
    "\n",
    "    # 5️⃣ Predict on dev (UNLABELED)\n",
    "    print(f\" Predicting for dev set ({len(dev_df)} rows)...\")\n",
    "    dev_dataset = PolarizationDataset(dev_df, tokenizer, require_labels=False)\n",
    "    preds = trainer.predict(dev_dataset)\n",
    "    pred_labels = np.argmax(preds.predictions, axis=1)\n",
    "\n",
    "    dev_df[\"predicted_polarization\"] = pred_labels\n",
    "    predicted_outputs[lang] = dev_df\n",
    "\n",
    "# ---------------------------------------\n",
    "# SAVE PREDICTIONS\n",
    "# ---------------------------------------\n",
    "for lang, df_pred in predicted_outputs.items():\n",
    "    df_pred.to_csv(f\"{lang}_dev_predicted.csv\", index=False)\n",
    "    print(f\" Saved: {lang}_dev_predicted.csv\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# FINAL F1 SCORES\n",
    "# ---------------------------------------\n",
    "f1_df = pd.DataFrame(f1_results)\n",
    "print(\"\\n FINAL F1 SCORES:\")\n",
    "print(f1_df)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
